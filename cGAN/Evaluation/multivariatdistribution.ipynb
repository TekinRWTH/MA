import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torchvision.models.inception import Inception_V3_Weights
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
import numpy as np
import os
from PIL import Image
from scipy import linalg
from sklearn.decomposition import PCA


# =====================================
# USER INPUTS - MODIFY THESE VARIABLES
# =====================================
DATASET1_PATH = "/Users/deniztekin/Documents/Uni/Masterarbeit/inference/681/y_gen/"
DATASET2_PATH = "/Users/deniztekin/Documents/Uni/Masterarbeit/inference/681/label/"
OUTPUT_STATS_PATH = "tiff_distribution_statistics.npz"
OUTPUT_SVG_PATH = "tiff_distribution_comparison.svg"
BATCH_SIZE = 16
NUM_WORKERS = 0
# =====================================


class TiffImageDataset(Dataset):
    def __init__(self, img_dir, transform=None):
        self.img_dir = img_dir
        self.transform = transform
        self.file_list = [f for f in os.listdir(img_dir) 
                         if f.lower().endswith(('.tiff', '.tif'))]
        
    def __len__(self):
        return len(self.file_list)
    
    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.file_list[idx])
        try:
            image = Image.open(img_path)
            if image.mode != 'RGB':
                image = image.convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image
        except Exception as e:
            print(f"Error loading {img_path}: {e}")
            blank = Image.new('RGB', (299, 299), (0, 0, 0))
            return self.transform(blank) if self.transform else blank


class InceptionV3FeatureExtractor(nn.Module):
    def __init__(self):
        super().__init__()
        self.inception = models.inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)
        self.inception.fc = nn.Identity()
        self.inception.aux_logits = False
        self.inception.eval()
        
    def forward(self, x):
        return self.inception(x)


def extract_features(model, dataloader, device):
    features = []
    with torch.no_grad():
        for batch_idx, images in enumerate(dataloader):
            print(f"Processing batch {batch_idx+1}/{len(dataloader)}")
            try:
                batch_features = model(images.to(device))
                features.append(batch_features.cpu().numpy())
            except Exception as e:
                print(f"Error in batch {batch_idx}: {e}")
    return np.concatenate(features, axis=0)


def calculate_statistics(features):
    return np.mean(features, axis=0), np.cov(features, rowvar=False)


def plot_2d_distributions(features1, features2, save_path):
    """Grayscale version with pattern differentiation"""
    # Combine and reduce dimensionality
    combined_features = np.vstack([features1, features2])
    pca = PCA(n_components=2)
    pca.fit(combined_features)
    
    # Transform both datasets
    reduced1 = pca.transform(features1)
    reduced2 = pca.transform(features2)
    
    # Calculate statistics in PCA space
    mu1, sigma1 = np.mean(reduced1, axis=0), np.cov(reduced1, rowvar=False)
    mu2, sigma2 = np.mean(reduced2, axis=0), np.cov(reduced2, rowvar=False)

    # Visualization parameters
    STYLE1 = {'color': '#202020', 'linestyle': '-', 'marker': 'o', 'hatch': '///'}
    STYLE2 = {'color': '#808080', 'linestyle': '--', 'marker': 's', 'hatch': 'xxx'}
    
    plt.figure(figsize=(10, 8))
    ax = plt.gca()
    
    for (mu, sigma, style), label in zip([(mu1, sigma1, STYLE1), 
                                        (mu2, sigma2, STYLE2)], 
                                       ['Datensatz 1', 'Datensatz 2']):
        vals, vecs = np.linalg.eigh(sigma)
        theta = np.degrees(np.arctan2(vecs[1,0], vecs[0,0]))
        w, h = 2 * np.sqrt(vals)
        
        # Ellipses with different line styles and patterns
        for nstd in [1, 2, 3]:
            ell = Ellipse(mu, width=w*nstd, height=h*nstd, 
                          angle=theta, alpha=0.15*(4-nstd),
                          edgecolor=style['color'],
                          facecolor='none',
                          linestyle=style['linestyle'],
                          linewidth=1.5,
                          hatch=style['hatch'])
            ax.add_patch(ell)
            
        # Mean markers with different shapes
        plt.scatter(*mu, c=style['color'], s=150, 
                    label=label, marker=style['marker'],
                    edgecolors='k', linewidths=1,
                    zorder=3)  # Ensure markers stay on top

    plt.title('Vergleich der Merkmalsverteilung')
    plt.xlabel('Hauptkomponente 1')
    plt.ylabel('Hauptkomponente 2')
    
    # Styling adjustments
    ax.set_facecolor('#f0f0f0')  # Light background
    plt.grid(color='#c0c0c0', linestyle=':', linewidth=0.8)
    
    # Legend customization
    leg = plt.legend(framealpha=0.9, edgecolor='#404040')
    for text in leg.get_texts():
        text.set_color('#404040')  # Dark text

    plt.tight_layout()
    plt.savefig(save_path, format='svg', bbox_inches='tight')
    plt.close()


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    transform = transforms.Compose([
        transforms.Resize((299, 299)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    # Create datasets and loaders
    dataset1 = TiffImageDataset(DATASET1_PATH, transform)
    dataset2 = TiffImageDataset(DATASET2_PATH, transform)
    dataloader1 = DataLoader(dataset1, batch_size=BATCH_SIZE, 
                            shuffle=False, num_workers=NUM_WORKERS)
    dataloader2 = DataLoader(dataset2, batch_size=BATCH_SIZE, 
                            shuffle=False, num_workers=NUM_WORKERS)

    # Feature extraction
    model = InceptionV3FeatureExtractor().to(device)
    print("Extracting features from dataset 1...")
    features1 = extract_features(model, dataloader1, device)
    print("Extracting features from dataset 2...")
    features2 = extract_features(model, dataloader2, device)

    # Save statistics
    mu1, sigma1 = calculate_statistics(features1)
    mu2, sigma2 = calculate_statistics(features2)
    np.savez(OUTPUT_STATS_PATH, mu1=mu1, sigma1=sigma1, mu2=mu2, sigma2=sigma2)

    # Generate visualization
    print("Creating grayscale comparison...")
    plot_2d_distributions(features1, features2, OUTPUT_SVG_PATH)
    print(f"Results saved to:\n{OUTPUT_STATS_PATH}\n{OUTPUT_SVG_PATH}")


if __name__ == "__main__":
    main()
